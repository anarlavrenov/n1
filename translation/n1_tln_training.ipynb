{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMPw8Pw3xeamQZYXBz0fAaC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anarlavrenov/n1/blob/main/n1_tln_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYsiREfQQyEx",
        "outputId": "dd9b83c0-aa72-44f0-8300-5fffb2efb36a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "rAEKG0NQWipn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "def init_packages() -> None:\n",
        "\n",
        "  functions_path = \"/PATH_TO_YOUR_PROJECT\"\n",
        "  sys.path.append(functions_path)\n",
        "\n",
        "init_packages()"
      ],
      "metadata": {
        "id": "_K2bVHkXRTB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import locale\n",
        "# locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!pip install datasets --quiet\n",
        "\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from typing import Tuple, List\n",
        "\n",
        "def create_dataset(n_train_samples: int, n_valid_samples: int) -> Tuple[List, List]:\n",
        "\n",
        "  dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-uk\", split=\"train[:30%]\")\n",
        "\n",
        "  train_df = pd.DataFrame(dataset[\"translation\"])[:n_train_samples]\n",
        "  valid_df = pd.DataFrame(dataset[\"translation\"])[n_train_samples: n_train_samples + n_valid_samples]\n",
        "\n",
        "  return train_df, valid_df"
      ],
      "metadata": {
        "id": "FL4pD5OXcE_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ed055d6-2315-4fe0-d2d2-fde070469132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/510.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m337.9/510.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(row: str) -> str:\n",
        "\n",
        "  row = re.sub(r'https?:\\/\\/\\S+|www\\.[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]+', '', row)\n",
        "  row = re.sub(r'\\s+', ' ', row).strip()\n",
        "  row = row.replace(\"/\", \"\").strip()\n",
        "  row = row.replace(\")\", \"\").strip()\n",
        "  row = row.replace(\"(\", \"\").strip()\n",
        "\n",
        "  return row"
      ],
      "metadata": {
        "id": "m131mCGby07N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import PositionalEncoding\n",
        "import math\n",
        "\n",
        "class Encoder(torch.nn.Module):\n",
        "  def __init__(self, num_layers: int, d_model: int, nhead: int,\n",
        "               dff: int, ntokens_src: int, dropout: float = 0.5):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.embedding = torch.nn.Embedding(num_embeddings=ntokens_src,\n",
        "                                        embedding_dim=d_model,\n",
        "                                        padding_idx=0)\n",
        "\n",
        "    self.pos_encoding = PositionalEncoding(d_model=d_model,\n",
        "                                           dropout=dropout)\n",
        "\n",
        "    encoder_layer = torch.nn.TransformerEncoderLayer(d_model=d_model,\n",
        "                                                           nhead=nhead,\n",
        "                                                           dim_feedforward=dff,\n",
        "                                                           dropout=dropout,\n",
        "                                                           norm_first=True)\n",
        "\n",
        "    self.encoder = torch.nn.TransformerEncoder(encoder_layer=encoder_layer,\n",
        "                                               num_layers=num_layers)\n",
        "\n",
        "\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.linear_glu = torch.nn.Linear(in_features=d_model,\n",
        "                    out_features=d_model * 2)\n",
        "\n",
        "  def forward(self, src: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
        "    # src -> src_seq_len, batch_size\n",
        "    src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "    src = self.pos_encoding(src)\n",
        "\n",
        "    src = torch.nn.functional.glu(self.linear_glu(src), dim=-1) # Застосування GLU\n",
        "\n",
        "    if mask is None:\n",
        "      mask = torch.nn.Transformer.generate_square_subsequent_mask(sz=len(src)).to(device)\n",
        "\n",
        "    encoder_output = self.encoder(src, mask)\n",
        "\n",
        "    return encoder_output # -> Tensor shape: src_seq_len, batch_size, d_model\n",
        "\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "  def __init__(self, num_layers: int, d_model: int, nhead: int,\n",
        "               dff: int, ntokens_tgt: int, dropout: float = 0.5):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.embedding = torch.nn.Embedding(num_embeddings=ntokens_tgt,\n",
        "                                        embedding_dim=d_model,\n",
        "                                        padding_idx=0)\n",
        "\n",
        "    self.pos_encoding = PositionalEncoding(d_model=d_model,\n",
        "                                           dropout=dropout)\n",
        "\n",
        "    decoder_layer = torch.nn.TransformerDecoderLayer(d_model=d_model,\n",
        "                                                      nhead=nhead,\n",
        "                                                      dim_feedforward=dff,\n",
        "                                                      dropout=dropout,\n",
        "                                                      norm_first=True)\n",
        "\n",
        "    self.decoder = torch.nn.TransformerDecoder(decoder_layer=decoder_layer,\n",
        "                                               num_layers=num_layers)\n",
        "\n",
        "\n",
        "    self.fc = torch.nn.Linear(in_features=d_model,\n",
        "                              out_features=ntokens_tgt)\n",
        "\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.linear_glu = torch.nn.Linear(in_features=d_model,\n",
        "                    out_features=d_model * 2)\n",
        "\n",
        "  def forward(self, tgt: torch.Tensor, memory: torch.Tensor,\n",
        "              tgt_mask: torch.Tensor = None, memory_mask: torch.Tensor = None):\n",
        "\n",
        "    tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
        "    tgt = self.pos_encoding(tgt)\n",
        "\n",
        "    tgt = torch.nn.functional.glu(self.linear_glu(tgt), dim=-1) # Застосування GLU\n",
        "\n",
        "    if tgt_mask is None:\n",
        "      tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(len(tgt)).to(device)\n",
        "\n",
        "    if memory_mask is None:\n",
        "      memory_mask = torch.zeros((tgt.size(1), memory.size(0))).to(device)\n",
        "\n",
        "    decoder_output = self.decoder(tgt, memory,\n",
        "                                  tgt_mask=tgt_mask, memory_key_padding_mask=memory_mask)\n",
        "\n",
        "\n",
        "    output = self.fc(decoder_output) # -> Tensor shape: tgt_seq_len, batch_size, ntokens\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "class Transformer(torch.nn.Module):\n",
        "  def __init__(self, num_layers_encoder: int, num_layers_decoder: int, d_model: int, nhead: int,\n",
        "               dff: int, ntokens_src: int, ntokens_tgt: int, dropout: float = 0.5):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers_encoder, d_model, nhead, dff, ntokens_src)\n",
        "    self.decoder = Decoder(num_layers_decoder, d_model, nhead, dff, ntokens_tgt)\n",
        "\n",
        "\n",
        "  def forward(self, src: torch.Tensor, tgt: torch.Tensor):\n",
        "\n",
        "    memory = self.encoder(src)\n",
        "    decoder_output = self.decoder(tgt, memory)\n",
        "\n",
        "    return decoder_output"
      ],
      "metadata": {
        "id": "AVhybPOARt1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, valid_df = create_dataset(n_train_samples=150000,\n",
        "                                    n_valid_samples=1000)"
      ],
      "metadata": {
        "id": "LDbOK-ZTbXJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.rename(columns={\"en\": \"EN\", \"uk\": \"UK\"})\n",
        "valid_df = valid_df.rename(columns={\"en\": \"EN\", \"uk\": \"UK\"})"
      ],
      "metadata": {
        "id": "E6yFQWhe36B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Лімітування довжин текстів через квантиль 90 відсотків для запобігання вибросам\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "maxlen_uk = int(np.quantile([len(x.split()) for x in train_df[\"UK\"]], q=0.9))\n",
        "maxlen_en = int(np.quantile([len(x.split()) for x in train_df[\"EN\"]], q=0.9))\n",
        "\n",
        "train_df = train_df[train_df[\"UK\"].str.split().str.len() < maxlen_uk]\n",
        "train_df = train_df[train_df[\"EN\"].str.split().str.len() < maxlen_en]\n",
        "\n",
        "valid_df = valid_df[valid_df[\"UK\"].str.split().str.len() < maxlen_uk]\n",
        "valid_df = valid_df[valid_df[\"EN\"].str.split().str.len() < maxlen_en]"
      ],
      "metadata": {
        "id": "wxF6TupYVRhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"EN\"] = [preprocess_text(x) for x in train_df[\"EN\"]]\n",
        "train_df[\"UK\"] = [preprocess_text(x) for x in train_df[\"UK\"]]\n",
        "\n",
        "\n",
        "valid_df[\"EN\"] = [preprocess_text(x) for x in valid_df[\"EN\"]]\n",
        "valid_df[\"UK\"] = [preprocess_text(x) for x in valid_df[\"UK\"]]"
      ],
      "metadata": {
        "id": "h8n01tRrRr-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape, maxlen_uk, maxlen_en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Jrt9HRgVItI",
        "outputId": "6ce34795-b3e6-4b61-cf42-c7f5bd21ed1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((392780, 2), 16, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Формування функцій токенизації текстів\n",
        "\n",
        "# !python -m spacy download uk_core_news_trf\n",
        "# !python -m spacy download en_core_web_trf\n",
        "\n",
        "import spacy\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from typing import Callable\n",
        "\n",
        "\n",
        "def tokenize(input_data: List[str], nlp) -> torch.Tuple[Callable[[str], List[str]], torchtext.vocab.Vocab]:\n",
        "\n",
        "  def tokenizer(text: str) -> List[str]:\n",
        "    return [tok.text for tok in nlp.tokenizer(text)]\n",
        "\n",
        "  data_iter = iter(input_data)\n",
        "  vocab = build_vocab_from_iterator(map(tokenizer, data_iter), specials=[\"<unk>\"])\n",
        "  vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "  return tokenizer, vocab\n",
        "\n",
        "spacy.prefer_gpu()\n",
        "tokenizer_uk, vocab_uk = tokenize(train_df[\"UK\"], nlp=spacy.load(\"uk_core_news_trf\"))\n",
        "tokenizer_en, vocab_en = tokenize(train_df[\"EN\"], nlp=spacy.load(\"en_core_web_trf\"))"
      ],
      "metadata": {
        "id": "zSdapTpydlff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e91de95c-bb50-458b-c16b-08eaf37a3610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Формування датасету PyTorch\n",
        "\n",
        "class DataWrapper(torch.utils.data.Dataset):\n",
        "  def __init__(self, uk: List[str], en: List[str]):\n",
        "    super(DataWrapper, self).__init__()\n",
        "\n",
        "    start_token_uk = [len(vocab_uk)]\n",
        "    end_token_uk = [len(vocab_uk) + 1]\n",
        "\n",
        "    start_token_en = [len(vocab_en)]\n",
        "    end_token_en = [len(vocab_en) + 1]\n",
        "\n",
        "    self.uk = uk\n",
        "    self.en = en\n",
        "\n",
        "    self.uk_ = [vocab_uk(tokenizer_uk(word)) for word in self.uk]\n",
        "    self.en_ = [vocab_en(tokenizer_en(word)) for word in self.en]\n",
        "\n",
        "    self.uk_ = np.asarray([self.pad_sequences(seq, maxlen_uk,\n",
        "                                                start_token_uk, end_token_uk) for seq in self.uk_])\n",
        "    self.en_ = np.asarray([self.pad_sequences(seq, maxlen_en,\n",
        "                                                 start_token_en, end_token_en) for seq in self.en_])\n",
        "\n",
        "  def __len__(self):\n",
        "\n",
        "    return len(self.uk_)\n",
        "\n",
        "\n",
        "  def __getitem__(self, index: int):\n",
        "    return self.uk_[index], self.en_[index]\n",
        "\n",
        "\n",
        "  def pad_sequences(self, seq, max_len: int, start_token, end_token):\n",
        "    if max_len > len(seq):\n",
        "      padding = [0] * (max_len - len(seq))\n",
        "\n",
        "      return start_token + seq + end_token + padding\n",
        "\n",
        "    else:\n",
        "      return start_token + seq[:max_len] + end_token"
      ],
      "metadata": {
        "id": "m0fWaGbMgnz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DataWrapper(train_df[\"UK\"],\n",
        "                            train_df[\"EN\"])\n",
        "\n",
        "valid_dataset = DataWrapper(valid_df[\"UK\"],\n",
        "                            valid_df[\"EN\"])"
      ],
      "metadata": {
        "id": "wZoQHCxfh2MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Формування даталоадеру PyTorch\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=128,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=2,\n",
        "                                           drop_last=True)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
        "                                           batch_size=16,\n",
        "                                           shuffle=False,\n",
        "                                           num_workers=2,\n",
        "                                           drop_last=True)"
      ],
      "metadata": {
        "id": "_oMr9UePoYL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ініціалізація трансформеру\n",
        "\n",
        "num_layers_encoder = 2\n",
        "num_layers_decoder = 2\n",
        "d_model = 256\n",
        "nhead = 8\n",
        "dff = 512\n",
        "ntokens_src = len(vocab_uk) + 2\n",
        "ntokens_tgt = len(vocab_en) + 2\n",
        "dropout = 0.5\n",
        "\n",
        "model = Transformer(num_layers_encoder, num_layers_decoder,\n",
        "                    d_model, nhead, dff, ntokens_src, ntokens_tgt, dropout=dropout)\n",
        "\n",
        "for param in model.parameters():\n",
        "  if param.dim() > 1:\n",
        "    torch.nn.init.xavier_uniform_(param)\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "dE8XM2uFo6H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src = next(iter(train_loader))[0].long().to(device)\n",
        "tgt = next(iter(train_loader))[1].long().to(device)"
      ],
      "metadata": {
        "id": "J64RqHFQa7i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\" Вихідний розмір прогноза трансформеру: {dec_res.shape}, Початковий розмір таргету: {tgt.permute(1, 0).shape} \\n\"\n",
        "      f\" Такий розмір повинен мати pred: {dec_res.view(-1, ntokens).shape} \"\n",
        "      f\"і таргет: {tgt.permute(1, 0).reshape(-1).shape} для функциї CrossEntropy\")"
      ],
      "metadata": {
        "id": "pG_qfwp1dCZX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30630b7b-d911-4bd2-a24a-3d0fbc1cb9ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Выход прогноза модели имеет размерность: torch.Size([21, 128, 143400]), изначальная размерность таргета: torch.Size([21, 128]) \n",
            " Такую размерность должен иметь pred: torch.Size([2688, 143400]) и таргет: torch.Size([2688]) для функции CrossEntropy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.1\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = torch.optim.AdamW(model.parameters())\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.9)"
      ],
      "metadata": {
        "id": "B_9Rfeu0ony2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функція інференсу після навчання моделі\n",
        "\n",
        "def summarize(string: str, model: torch.nn.Module,\n",
        "              repetition_penalty: float = 1.2) -> torch.Tensor:\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  start_token_uk = [len(vocab_uk)]\n",
        "  end_token_uk = [len(vocab_uk) + 1]\n",
        "\n",
        "  start_token_en = [len(vocab_en)]\n",
        "  end_token_en = [len(vocab_en) + 1]\n",
        "\n",
        "  string = torch.IntTensor(start_token_uk + [vocab_uk(tokenizer_uk(word))[0] for word in string.split()] + end_token_uk).unsqueeze(0).to(device)\n",
        "  output = torch.IntTensor(start_token_en).unsqueeze(0).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for i in range(maxlen_en):\n",
        "\n",
        "      prediction = model(string.permute(1, 0), output.permute(1, 0))\n",
        "\n",
        "      prediction = prediction[-1:, :, :]\n",
        "\n",
        "      if i > 1:\n",
        "        # repetition penalty\n",
        "        for token_id in set(output.squeeze().tolist()):\n",
        "          prediction[0, 0, token_id] /= repetition_penalty\n",
        "\n",
        "      predicted_id = torch.argmax(prediction, dim=-1)\n",
        "\n",
        "      if predicted_id[0] == end_token_en[0]:\n",
        "        return output.squeeze(0)\n",
        "\n",
        "      output = torch.cat([output, predicted_id.permute(1, 0)], dim=-1)\n",
        "\n",
        "    return output.squeeze(0)"
      ],
      "metadata": {
        "id": "iItZXGzuCwXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функції навчання моделі на трейні та валідації\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(loader: torch.Tensor) -> float:\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "  for batch in tqdm(loader):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    src, tgt = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "    tgt_inp = tgt[:, :-1].permute(1, 0)\n",
        "    tgt_real = tgt[:, 1:].permute(1, 0)\n",
        "\n",
        "    outputs = model(src.permute(1, 0), tgt_inp)\n",
        "    loss = criterion(outputs.view(-1, ntokens_tgt), tgt_real.reshape(-1))\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.7)\n",
        "    optimizer.step()\n",
        "\n",
        "  return total_loss / len(loader)\n",
        "\n",
        "\n",
        "def eval_(loader: torch.Tensor) -> float:\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch in tqdm(loader):\n",
        "\n",
        "      src, tgt = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "      tgt_inp = tgt[:, :-1].permute(1, 0)\n",
        "      tgt_real = tgt[:, 1:].permute(1, 0)\n",
        "\n",
        "      outputs = model(src.permute(1, 0), tgt_inp)\n",
        "      loss = criterion(outputs.view(-1, ntokens_tgt), tgt_real.reshape(-1))\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "  res = summarize(valid_df[\"UK\"].iloc[15], model=model)\n",
        "\n",
        "  # Відпринтовування поточного результату інференса моделі на даній епосі навчання\n",
        "  print(\" \".join([vocab_en.get_itos()[word] for word in res[1:]]))\n",
        "\n",
        "  return total_loss / len(loader)"
      ],
      "metadata": {
        "id": "QFMK1IwPc7dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Запуск циклу навчання трансформерної моделі\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  loss = train(train_loader)\n",
        "  valid_loss = eval_(valid_loader)\n",
        "  print(f\"epoch: {epoch + 1} | loss: {loss:.3f} | valid_loss: {valid_loss:.3f}\")\n",
        "\n",
        "  scheduler.step()"
      ],
      "metadata": {
        "id": "muGS1yx6khPH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b1f426e-3a00-41a2-e3d8-939d1b1358d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3068/3068 [04:29<00:00, 11.36it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 56.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The external operation and implementation of the educational programmes shall be carried out in the main principles of those\n",
            "epoch: 1 | loss: 4.464 | valid_loss: 3.345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3068/3068 [04:30<00:00, 11.33it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 56.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The operation and implementation of the National Register shall be carried out with the basic principles .\n",
            "epoch: 2 | loss: 3.122 | valid_loss: 2.865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3068/3068 [04:30<00:00, 11.34it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 56.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The management and implementation of the National programmes shall be carried out with such basic principles .\n",
            "epoch: 3 | loss: 2.535 | valid_loss: 2.715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3068/3068 [04:30<00:00, 11.35it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 55.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formation and implementation of the National Programme shall be performed with the following basic principles :\n",
            "epoch: 4 | loss: 2.160 | valid_loss: 2.667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3068/3068 [04:30<00:00, 11.36it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 57.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formation and implementation of the National Programme shall be carried out with such principles .\n",
            "epoch: 5 | loss: 1.896 | valid_loss: 2.656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Перевірка інференсу\n",
        "\n",
        "res = summarize(valid_df[\"UK\"].iloc[330], model=model)\n",
        "\n",
        "\" \".join([vocab_en.get_itos()[word] for word in res[1:]])"
      ],
      "metadata": {
        "id": "Gvokgwi7DHYM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "91125e14-2488-472c-877a-36614acfc079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Documents confirming his parents who supported him wash their rivals .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_df[\"EN\"].iloc[330]"
      ],
      "metadata": {
        "id": "0_P9mCdmdQqc",
        "outputId": "f58d1dcb-b253-423f-f532-10f58ea9edf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'documents confirming their legal succession;'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Зберігання результатів\n",
        "\n",
        "import dill\n",
        "\n",
        "torch.save(model, \"/YOUR_PROJECT_PATH/model.pth\")\n",
        "\n",
        "torch.save(optimizer.state_dict(), \"/YOUR_PROJECT_PATH/optimizer_state_dict.pth\")\n",
        "\n",
        "with open(\"/YOUR_PROJECT_PATH/vocab_en.pkl\", \"wb\") as f:\n",
        "  dill.dump(vocab_en, f)\n",
        "\n",
        "\n",
        "with open(\"/YOUR_PROJECT_PATH/vocab_uk.pkl\", \"wb\") as f:\n",
        "  dill.dump(vocab_uk, f)"
      ],
      "metadata": {
        "id": "CRCQQqd9kmNZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
