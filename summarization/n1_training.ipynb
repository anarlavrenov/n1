{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNuRAPr3JDZOZs3kwxaKKbz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anarlavrenov/n1/blob/master/n1_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYsiREfQQyEx",
        "outputId": "c79a2c7f-7c2a-4fa4-983a-490d01dee883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "rAEKG0NQWipn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "def init_packages() -> None:\n",
        "\n",
        "  functions_path = \"/PATH_TO_YOUR_PROJECT\"\n",
        "  sys.path.append(functions_path)\n",
        "\n",
        "init_packages()"
      ],
      "metadata": {
        "id": "_K2bVHkXRTB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets --quiet\n",
        "\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from typing import Tuple, List\n",
        "\n",
        "def create_dataset(n_train_samples: int, n_valid_samples: int) -> Tuple[List, List]:\n",
        "\n",
        "  train_dataset = load_dataset(\"d0p3/ukr-pravda-news-summary\", split=\"train\")\n",
        "\n",
        "  train_df = pd.DataFrame(train_dataset)[:n_train_samples]\n",
        "  valid_df = pd.DataFrame(train_dataset)[n_train_samples: n_train_samples + n_valid_samples]\n",
        "\n",
        "  return train_df, valid_df"
      ],
      "metadata": {
        "id": "FL4pD5OXcE_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "847ced20-0fb9-4b31-99ec-b97f53cb6543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(row: str) -> str:\n",
        "\n",
        "  row = re.sub(r'https?:\\/\\/\\S+|www\\.[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]+', '', row)\n",
        "  row = re.sub(r'\\s+', ' ', row).strip()\n",
        "\n",
        "  return row"
      ],
      "metadata": {
        "id": "m131mCGby07N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import PositionalEncoding\n",
        "import math\n",
        "\n",
        "class Encoder(torch.nn.Module):\n",
        "  def __init__(self, num_layers: int, d_model: int, nhead: int,\n",
        "               dff: int, ntokens: int, dropout: float = 0.5):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.embedding = torch.nn.Embedding(num_embeddings=ntokens,\n",
        "                                        embedding_dim=d_model,\n",
        "                                        padding_idx=0)\n",
        "\n",
        "    self.pos_encoding = PositionalEncoding(d_model=d_model,\n",
        "                                           dropout=dropout)\n",
        "\n",
        "    encoder_layer = torch.nn.TransformerEncoderLayer(d_model=d_model,\n",
        "                                                           nhead=nhead,\n",
        "                                                           dim_feedforward=dff,\n",
        "                                                           dropout=dropout,\n",
        "                                                           norm_first=True)\n",
        "\n",
        "    self.encoder = torch.nn.TransformerEncoder(encoder_layer=encoder_layer,\n",
        "                                               num_layers=num_layers)\n",
        "\n",
        "\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.linear_glu = torch.nn.Linear(in_features=d_model,\n",
        "                    out_features=d_model * 2)\n",
        "\n",
        "  def forward(self, src: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
        "    # src -> seq_len, batch_size, d_model\n",
        "    src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "    src = self.pos_encoding(src)\n",
        "\n",
        "    src = torch.nn.functional.glu(self.linear_glu(src), dim=-1) # Застосування GLU\n",
        "\n",
        "    if mask is None:\n",
        "      mask = torch.nn.Transformer.generate_square_subsequent_mask(sz=len(src)).to(device)\n",
        "\n",
        "    encoder_output = self.encoder(src, mask)\n",
        "\n",
        "    return encoder_output # -> Tensor shape: seq_len, batch_size, ntokens\n",
        "\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "  def __init__(self, num_layers: int, d_model: int, nhead: int,\n",
        "               dff: int, ntokens: int, dropout: float = 0.5):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.embedding = torch.nn.Embedding(num_embeddings=ntokens,\n",
        "                                        embedding_dim=d_model,\n",
        "                                        padding_idx=0)\n",
        "\n",
        "    self.pos_encoding = PositionalEncoding(d_model=d_model,\n",
        "                                           dropout=dropout)\n",
        "\n",
        "    decoder_layer = torch.nn.TransformerDecoderLayer(d_model=d_model,\n",
        "                                                      nhead=nhead,\n",
        "                                                      dim_feedforward=dff,\n",
        "                                                      dropout=dropout,\n",
        "                                                      norm_first=True)\n",
        "\n",
        "    self.decoder = torch.nn.TransformerDecoder(decoder_layer=decoder_layer,\n",
        "                                               num_layers=num_layers)\n",
        "\n",
        "\n",
        "    self.fc = torch.nn.Linear(in_features=d_model,\n",
        "                              out_features=ntokens)\n",
        "\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.linear_glu = torch.nn.Linear(in_features=d_model,\n",
        "                    out_features=d_model * 2)\n",
        "\n",
        "  def forward(self, tgt: torch.Tensor, memory: torch.Tensor,\n",
        "              tgt_mask: torch.Tensor = None, memory_mask: torch.Tensor = None):\n",
        "\n",
        "    tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
        "    tgt = self.pos_encoding(tgt)\n",
        "\n",
        "    tgt = torch.nn.functional.glu(self.linear_glu(tgt), dim=-1) # Застосування GLU\n",
        "\n",
        "    if tgt_mask is None:\n",
        "      tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(len(tgt)).to(device)\n",
        "\n",
        "    if memory_mask is None:\n",
        "      memory_mask = torch.zeros((tgt.size(1), memory.size(0))).to(device)\n",
        "\n",
        "    decoder_output = self.decoder(tgt, memory,\n",
        "                                  tgt_mask=tgt_mask, memory_key_padding_mask=memory_mask)\n",
        "\n",
        "\n",
        "    output = self.fc(decoder_output)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "class Transformer(torch.nn.Module):\n",
        "  def __init__(self, num_layers_encoder: int, num_layers_decoder: int, d_model: int, nhead: int,\n",
        "               dff: int, ntokens: int, dropout: float = 0.5):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers_encoder, d_model, nhead, dff, ntokens)\n",
        "    self.decoder = Decoder(num_layers_decoder, d_model, nhead, dff, ntokens)\n",
        "\n",
        "\n",
        "  def forward(self, src: torch.Tensor, tgt: torch.Tensor):\n",
        "\n",
        "    memory = self.encoder(src)\n",
        "    decoder_output = self.decoder(tgt, memory)\n",
        "\n",
        "    return decoder_output"
      ],
      "metadata": {
        "id": "AVhybPOARt1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, valid_df = create_dataset(n_train_samples=50000,\n",
        "                                    n_valid_samples=5000)"
      ],
      "metadata": {
        "id": "LDbOK-ZTbXJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Лімітування довжин текстів через квантиль 80 відсотків для запобігання вибросам\n",
        "import numpy as np\n",
        "\n",
        "maxlen_text = int(np.quantile([len(x.split()) for x in train_df[\"text\"]], q=0.8))\n",
        "maxlen_title = int(np.quantile([len(x.split()) for x in train_df[\"summary\"]], q=0.8))\n",
        "\n",
        "train_df = train_df[train_df[\"text\"].str.split().str.len() < maxlen_text]\n",
        "train_df = train_df[train_df[\"summary\"].str.split().str.len() < maxlen_title]\n",
        "\n",
        "valid_df = valid_df[valid_df[\"text\"].str.split().str.len() < maxlen_text]\n",
        "valid_df = valid_df[valid_df[\"summary\"].str.split().str.len() < maxlen_title]"
      ],
      "metadata": {
        "id": "wxF6TupYVRhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"text\"] = [preprocess_text(x) for x in train_df[\"text\"]]\n",
        "train_df[\"summary\"] = [preprocess_text(x) for x in train_df[\"summary\"]]\n",
        "\n",
        "\n",
        "valid_df[\"text\"] = [preprocess_text(x) for x in valid_df[\"text\"]]\n",
        "valid_df[\"summary\"] = [preprocess_text(x) for x in valid_df[\"summary\"]]"
      ],
      "metadata": {
        "id": "h8n01tRrRr-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape, maxlen_text, maxlen_title"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Jrt9HRgVItI",
        "outputId": "8da18d51-50d8-4386-929b-78ad3204a6a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32986, 2), 237, 33)"
            ]
          },
          "metadata": {},
          "execution_count": 375
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Формування функцій токенизації текстів\n",
        "\n",
        "# !python -m spacy download uk_core_news_trf\n",
        "\n",
        "import spacy\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from typing import Callable\n",
        "\n",
        "\n",
        "def tokenize(input_data: List[str]) -> torch.Tuple[Callable[[str], List[str]], torchtext.vocab.Vocab]:\n",
        "  spacy.prefer_gpu()\n",
        "  nlp = spacy.load(\"uk_core_news_trf\")\n",
        "\n",
        "  def tokenizer(text: str) -> List[str]:\n",
        "    return [tok.text for tok in nlp.tokenizer(text)]\n",
        "\n",
        "  data_iter = iter(input_data)\n",
        "  vocab = build_vocab_from_iterator(map(tokenizer, data_iter), specials=[\"<unk>\"])\n",
        "  vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "  return tokenizer, vocab\n",
        "\n",
        "\n",
        "tokenizer, vocab = tokenize(train_df[\"text\"] + train_df[\"summary\"])"
      ],
      "metadata": {
        "id": "zSdapTpydlff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Формування датасету PyTorch\n",
        "\n",
        "class DataWrapper(torch.utils.data.Dataset):\n",
        "  def __init__(self, text: List[str], title: List[str]):\n",
        "    super(DataWrapper, self).__init__()\n",
        "\n",
        "    start_token = [len(vocab)]\n",
        "    end_token = [len(vocab) + 1]\n",
        "\n",
        "    self.text = text\n",
        "    self.title = title\n",
        "\n",
        "    self.text_ = [vocab(tokenizer(word)) for word in self.text]\n",
        "    self.title_ = [vocab(tokenizer(word)) for word in self.title]\n",
        "\n",
        "    self.text_ = np.asarray([self.pad_sequences(seq, maxlen_text,\n",
        "                                                start_token, end_token) for seq in self.text_])\n",
        "    self.title_ = np.asarray([self.pad_sequences(seq, maxlen_title,\n",
        "                                                 start_token, end_token) for seq in self.title_])\n",
        "\n",
        "  def __len__(self):\n",
        "\n",
        "    return len(self.text_)\n",
        "\n",
        "\n",
        "  def __getitem__(self, index: int):\n",
        "    return self.text_[index], self.title_[index]\n",
        "\n",
        "\n",
        "  def pad_sequences(self, seq, max_len: int, start_token, end_token):\n",
        "    if max_len > len(seq):\n",
        "      padding = [0] * (max_len - len(seq))\n",
        "\n",
        "      return start_token + seq + end_token + padding\n",
        "\n",
        "    else:\n",
        "      return start_token + seq[:max_len] + end_token"
      ],
      "metadata": {
        "id": "m0fWaGbMgnz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DataWrapper(train_df[\"text\"],\n",
        "                            train_df[\"summary\"])\n",
        "\n",
        "valid_dataset = DataWrapper(valid_df[\"text\"],\n",
        "                            valid_df[\"summary\"])"
      ],
      "metadata": {
        "id": "wZoQHCxfh2MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Формування даталоадеру PyTorh\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=32,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=2,\n",
        "                                           drop_last=True)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
        "                                           batch_size=16,\n",
        "                                           shuffle=False,\n",
        "                                           num_workers=2,\n",
        "                                           drop_last=True)"
      ],
      "metadata": {
        "id": "_oMr9UePoYL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ініціалізація трансформеру\n",
        "num_layers_encoder = 6\n",
        "num_layers_decoder = 6\n",
        "d_model = 512\n",
        "nhead = 8\n",
        "dff = 1024\n",
        "ntokens = len(vocab) + 2\n",
        "dropout = 0.5\n",
        "\n",
        "model = Transformer(num_layers_encoder, num_layers_decoder,\n",
        "                    d_model, nhead, dff, ntokens, dropout=dropout)\n",
        "\n",
        "for param in model.parameters():\n",
        "  if param.dim() > 1:\n",
        "    torch.nn.init.xavier_uniform_(param)\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "dE8XM2uFo6H4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8a74a3e-2c71-4c06-bb53-f5c4dee6a322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src = next(iter(train_loader))[0].long().to(device)\n",
        "tgt = next(iter(train_loader))[1].long().to(device)"
      ],
      "metadata": {
        "id": "J64RqHFQa7i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_res = model.encoder(src.permute(1, 0))\n",
        "dec_res = model.decoder(tgt.permute(1, 0), enc_res)\n",
        "\n",
        "print(f\" Вихідний розмір прогноза трансформеру: {dec_res.shape}, Початковий розмір таргету: {tgt.permute(1, 0).shape} \\n\"\n",
        "      f\" Такий розмір повинен мати pred: {dec_res.view(-1, ntokens).shape} \"\n",
        "      f\"і таргет: {tgt.permute(1, 0).reshape(-1).shape} для функциї CrossEntropy\")"
      ],
      "metadata": {
        "id": "fQZxy5UebDM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbfa15be-b02c-42b9-cd52-26c1daa5541b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([239, 32, 512]), torch.Size([35, 32]))"
            ]
          },
          "metadata": {},
          "execution_count": 382
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.1\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = torch.optim.AdamW(model.parameters())\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.9)"
      ],
      "metadata": {
        "id": "B_9Rfeu0ony2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функція інференсу після навчання моделі\n",
        "\n",
        "def summarize(string: str, model: torch.nn.Module,\n",
        "              tokenizer: Callable[[str], List[str]], vocab: torchtext.vocab.Vocab,\n",
        "              repetition_penalty: float = 1.2) -> torch.Tensor:\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  start_token = [len(vocab)]\n",
        "  end_token = [len(vocab) + 1]\n",
        "\n",
        "  string = torch.IntTensor([vocab(tokenizer(word))[0] for word in string.split()]).unsqueeze(0).to(device)\n",
        "  output = torch.IntTensor(start_token).unsqueeze(0).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for i in range(maxlen_title):\n",
        "\n",
        "      prediction = model(string.permute(1, 0), output.permute(1, 0))\n",
        "\n",
        "      prediction = prediction[-1:, :, :]\n",
        "\n",
        "      if i > 1:\n",
        "        # repetition penalty\n",
        "        for token_id in set(output.squeeze().tolist()):\n",
        "          prediction[0, 0, token_id] /= repetition_penalty\n",
        "\n",
        "      predicted_id = torch.argmax(prediction, dim=-1)\n",
        "\n",
        "      if predicted_id[0] == end_token[0]:\n",
        "        return output.squeeze(0)\n",
        "\n",
        "      output = torch.cat([output, predicted_id.permute(1, 0)], dim=-1)\n",
        "\n",
        "    return output.squeeze(0)"
      ],
      "metadata": {
        "id": "iItZXGzuCwXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функції навчання моделі на трейні та валідації\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(loader: torch.Tensor) -> float:\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "  for batch in tqdm(loader):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    src, tgt = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "    tgt_inp = tgt[:, :-1].permute(1, 0)\n",
        "    tgt_real = tgt[:, 1:].permute(1, 0)\n",
        "\n",
        "    outputs = model(src.permute(1, 0), tgt_inp)\n",
        "    loss = criterion(outputs.view(-1, ntokens), tgt_real.reshape(-1))\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.7)\n",
        "    optimizer.step()\n",
        "\n",
        "  return total_loss / len(loader)\n",
        "\n",
        "\n",
        "def eval_(loader: torch.Tensor) -> float:\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch in tqdm(loader):\n",
        "\n",
        "      src, tgt = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "      tgt_inp = tgt[:, :-1].permute(1, 0)\n",
        "      tgt_real = tgt[:, 1:].permute(1, 0)\n",
        "\n",
        "      outputs = model(src.permute(1, 0), tgt_inp)\n",
        "      loss = criterion(outputs.view(-1, ntokens), tgt_real.reshape(-1))\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "  res = summarize(valid_df[\"text\"].iloc[15], model=model,\n",
        "                  tokenizer=tokenizer, vocab=vocab)\n",
        "  # Відпринтовування поточного результату інференса моделі на даній епосі навчання\n",
        "  print(\" \".join([vocab.get_itos()[word] for word in res[1:]]))\n",
        "\n",
        "  return total_loss / len(loader)"
      ],
      "metadata": {
        "id": "QFMK1IwPc7dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Запуск циклу навчання трансформерної моделі\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  loss = train(train_loader)\n",
        "  valid_loss = eval_(valid_loader)\n",
        "  print(f\"epoch: {epoch + 1} | loss: {loss:.3f} | valid_loss: {valid_loss:.3f}\")\n",
        "\n",
        "  scheduler.step()"
      ],
      "metadata": {
        "id": "muGS1yx6khPH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c203cb37-12a5-42bf-d4da-cfd0a4676a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1030/1030 [02:20<00:00,  7.31it/s]\n",
            "100%|██████████| 204/204 [00:05<00:00, 39.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Російські окупанти обстріляли Херсон , внаслідок чого двоє чоловіків отримали поранення .\n",
            "epoch: 1 | loss: 6.595 | valid_loss: 5.255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1030/1030 [02:20<00:00,  7.31it/s]\n",
            "100%|██████████| 204/204 [00:05<00:00, 39.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" За добу російські окупанти в тимчасово окупованому Криму , де вони не було знайдено ще один з яких були конфісковані у зв'язку із загрозою для України . \"\n",
            "epoch: 2 | loss: 4.738 | valid_loss: 4.723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1030/1030 [02:21<00:00,  7.30it/s]\n",
            "100%|██████████| 204/204 [00:05<00:00, 39.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Російські окупанти в тимчасово окупованому Луганську , щоб звинуватити в цьому не було втрачено понад 10 хвилин у війні проти України .\n",
            "epoch: 3 | loss: 3.809 | valid_loss: 4.496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1030/1030 [02:21<00:00,  7.30it/s]\n",
            "100%|██████████| 204/204 [00:05<00:00, 39.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Російські окупанти захопили в окупованому Донецьку , де 12 березня біля окупованого Криму та Луганську переповнені пораненими російськими окупантами .\n",
            "epoch: 4 | loss: 3.109 | valid_loss: 4.494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1030/1030 [02:21<00:00,  7.30it/s]\n",
            "100%|██████████| 204/204 [00:05<00:00, 39.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Російські окупанти захопили в окупованому місті Могоча обшуки у центрі міста Новоайдар на Харківщині .\n",
            "epoch: 5 | loss: 2.577 | valid_loss: 4.538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Перевірка інференсу\n",
        "res = summarize(valid_df[\"text\"].iloc[7], model=model, tokenizer=tokenizer, vocab=vocab)\n",
        "\n",
        "\" \".join([vocab.get_itos()[word] for word in res[1:]])"
      ],
      "metadata": {
        "id": "Gvokgwi7DHYM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "eb110f5a-1f86-4e5f-89a8-a6c00e720d68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Держави - члени НАТО оголосили про початок навчань українських льотчиків на винищувачах F-16 , які будуть передані в Румунії .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 411
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_df[\"summary\"].iloc[7]"
      ],
      "metadata": {
        "id": "XmizXNk72__W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "50689972-547e-43ac-9028-7b24e729bef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Держави, які входять до так званої \"коаліції винищувачів\", розглядають Румунію як можливе місце для навчання українських льотчиків керуванню винищувачами F-16.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 412
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Зберігання результатів\n",
        "\n",
        "import dill\n",
        "\n",
        "torch.save(model, \"/YOUR_PROJECT_PATH/model.pth\")\n",
        "\n",
        "torch.save(optimizer.state_dict(), \"/YOUR_PROJECT_PATH/optimizer_state_dict.pth\")\n",
        "\n",
        "with open(\"/YOUR_PROJECT_PATH/vocab.pkl\", \"wb\") as f:\n",
        "  dill.dump(vocab, f)"
      ],
      "metadata": {
        "id": "CRCQQqd9kmNZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}